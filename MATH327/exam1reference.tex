\documentclass[9pt]{article}
\usepackage{amsthm, amsmath, extsizes, multicol}
\usepackage[margin=0.25in]{geometry}
\newcommand*\mean[1]{\bar{#1}}
\setlength{\multicolsep}{2pt}
\begin{document}
    \noindent\textbf{Chapter 1} Introduction to Statistics and Data Analysis
        \begin{multicols}{2}
            \noindent\underline{Sample Mean:} Add all elements and divide by number of elements.\\
            \underline{Sample Median:} Middle element of ordered list, or mean of middle two.\\
            \underline{Trimmed Mean:} Eliminate largest and smallest percentage and then take the mean.\\
            \underline{Sample Variance:} $s^2 = \sum_{i=1}^{n}=\frac{(x_i-\mean{x})^2}{n-1}$.\\
            \underline{Degrees of Freedom:} $n-1$.\\
            \underline{Sample Standard Deviation:} $s = \sqrt{s^2}$.\\
            \underline{Continuous Data:} Value over a given range, countably infinite.\\
            \underline{Discrete Data:} Particular value, countable.\\
            \underline{Scatter Plot:} Just graph that shit.\\
            \underline{Stem-and-Leaf Plot:} Split values into stems and leaves. Easy way would be the ten's place for the stems, and one's place for the leaves.\\
            \underline{Reletive Frequency:} Divide the frequency (occurrences) by total number of elements.\\
            \underline{Histogram:} Mate.\\
            \underline{Box Plot:} Take the median of the entire data set (2nd quartile), then take the median of the set of data to the left (1st quartile), and the median of the set of data to the right (3rd quartile). Outliers are values outside the range of 1.5 times the IQR.\\
            \underline{IQR:} The interquartile range is the 3rd quartile minus the 1st quartile.
        \end{multicols}
    \noindent\textbf{Chapter 2} Probability
        \begin{multicols}{2}
            \noindent\underline{Sample Space:} Set of all possible outcomes.\\
            \underline{Event:} Subset of the sample space.\\
            \underline{Complement ($A'$):} Subset of all elements not in an event.\\
            \underline{Intersection ($A\cap B$)} Event containing elements that exist in both events.\\
            \underline{Mutually Exclusive:} Two events have no elements in common.\\
            \underline{Union ($A\cup B$)} Event containing all elements from both events.\\
            \underline{Multiplication Rule:} If something can be done $x$ ways, and for each way, something else can be done $y$ ways, both can be done $xy$ ways. This stacks for the generalized multiplication rule.\\
            \underline{Permutations:} Different order means different set. The number of permutations of $n$ objects is $n!$. The number of permutations of $n$ objects but taking $r$ at a time ($_nP_r$) is $\frac{n!}{(n-r)!}$.\\
            \underline{Circular Permutations:} Permutations for arranging objects in a circle. The number of permutations of $n$ objects in a circle is $(n-1)!$.\\
            \underline{Distinct Permutations:} Number of distinct permutations of $n$ things of which $n_1$ are of one kind, $n_2$ are another, and $n_k$ is the $k$th kind is $\frac{n!}{n_1!n_2!...n_k!}$.\\
            \underline{Combinations:} Different order means same set. The number of combinations of $n$ distinct objects raken $r$ at a time is $\binom{n}{r} = \frac{n!}{r!(n-r)!}$.\\
            \underline{Probability:} Probability of an event is the sum of all weights of all sample points in the event. Probability of an event is the number of outcomes corresponding to the event divided by total outcomes.\\
            \underline{Additive Rules:} If $A$ and $B$ are two events. $P(A\cup B) = P(A) + P(B) - P(A\cap B)$. $P(A\cup B\cup C) = P(A)+P(B)+P(C)-P(A\cap B) - P(A\cap C) - P(B\cap C) + P(A\cap B\cap C)$. If $A$ and $B$ are mutually exclusive, $P(A\cup B) = P(A) + P(B)$. The summation of all probabilities should equal 1. $P(A) + P(A') = 1$.\\
            \underline{Conditional Probability:} Probability of B given A is $P(B|A)=\frac{P(A\cap B)}{P(A)}$ provided $P(A) > 0$.\\
            \underline{Independent:} If $P(B|A) = P(B)$ then $A$ and $B$ are independent. If $P(A\cap B)=P(A)P(B)$\\
            \underline{Product Rules:} If $A$ and $B$ can both occur. $P(A\cap B) = P(A)P(B|A)$ provided $P(A)>0$. If $A_1,A_2...A_k$ can occur then $P(A_1\cap A_2\cap ... A_k) = P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)...P(A_k|A_1\cap A_2\cap ... A_{k-1})$.\\
            \underline{Theorem of Total Probability:} If the events $B_1, B_2,...,B_k$ are the partitions of the sample space $S$ such that $P(B_i)\not= 0$. then for any event $A$ of $S$, $P(A)=\sum_{i=1}^{k}P(B_i\cap A)=\sum_{i=1}^{k}P(B_i)P(A|B_i)$.\\
            \underline{Bayes' Rule:} If the events $B_1, B_2,...,B_k$ are the partitions of the sample space $S$ such that $P(B_i)\not= 0$. then for any event $A$ of $S$, $P(B_r|A) = \frac{P(B_r\cap A)}{\sum_{i=1}^{k}P(B_i\cap A)} = \frac{P(B_r)P(A|B_r)}{\sum_{i=1}^{k}P(B_i)P(A|B_i)}$.\\
        \end{multicols}
    \noindent\textbf{Chapter 3} Random Variables and Probability Distribution
        \begin{multicols}{2}
            \noindent\underline{Random Variable:} A function that associates a real number with each element in the sample space.\\
            \underline{Discrete Sample Space:} Sample space contains finite possibilities or countably infinitly many elements. Discrete random variable.\\
            \underline{Continuous Sample Space:} Sample space contains infinite possibilities. Continuous random variable.\\
            \underline{Probability Mass Function:} $f(x)$ for discrete random variable $X$ if $f(x) \geq 0$; $\sum_xf(x) = 1$; $P(X=x) = f(x)$.\\
            \underline{Probability Density Function:} $f(x)$ for continuous random variable $X$ if $f(x)\geq 0$ $\forall x\in R$; $\int_{-\infty}^{\infty}f(x)dx = 1$; $P(a<X<b) = \int_a^bf(x)dx$.
            \underline{Cumulative Distribution Function:} Find the probability that $X$ will be less than or equal to a real number. Piece together in a piecewise function. Discrete: $F(x) = P(X\leq x) = \sum_{t\leq x}f(t)$. Continuous: $F(x) = P(X\leq x) = \int_{-\infty}^{x}f(t)dt$.\\
            \underline{Joint Probability Distribution or Probability Mass Function:} Two random variables $X$ and $Y$. Discrete: $f(x,y)\geq 0$ $\forall(x,y)$; $\sum_x\sum_yf(x,y) = 1$; $P(X=x,Y=y)=f(x,y)$; For any region $A$ in the $xy$ plane: $P[(X,Y)\in A]=\sum\sum_af(x,y)$. Continous: $f(x,y)\geq 0$ $\forall(x,y)$; $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y)dxdy=1$; For any region $A$ in the $xy$ plane: $P[(X,Y)\in A]=\int\int_Af(x,y)dxdy$.\\
            \underline{Marginal Distributions:} Discrete: $g(x) = \sum_yf(x,y)$ and $h(y)=\sum_xf(x,y)$. Continous: $g(x) = \int_{-\infty}^{\infty}f(x,y)dy$ and $h(y)=\int_{-\infty}^{\infty}f(x,y)dx$.\\
            \underline{Conditional Distribution:} $f(y|x)=\frac{f(x,y)}{g(x)}$ or $f(x|y)=\frac{f(x,y)}{h(y)}$
            \underline{Statistically Independent:} If and only if $f(x,y) = g(x)h(y)$. This will stack with any number of variables.
        \end{multicols}
    \noindent\textbf{Chapter 4} Mathematical Expectation
        \begin{multicols}{2}
            \underline{Expected Value (Mean):} Discrete: $\mu=E(X) = \sum_xxf(x)$. Continous: $\mu=E(X)=\int_{-\infty}^{\infty}xf(x)dx$. This also works for $g(X)$ as $\mu_{g(X)} = E[g(X)] = \sum_xg(x)f(x)$. Also for joint probability: $\mu_{g(X,Y)}=E[g(X,Y)]=\sum_x\sum_yg(x,y)f(x,y)$. Same idea of continous. Also $E(ax+b) = aE(x)+b$\\
            \underline{Variance and Standard Deviation:} Discrete: $\sigma_x^2=E(x^2)-\mu_x^2=\sum_xx^2f(x)-\mu_x^2$. Continuous: $\sigma_x^2=E(x^2)-\mu_x^2=\int_{-\infty}^{\infty}x^2f(x)dx-\mu_x^2$. Can replace $x$ with any other variable including $g(x)$.\\
            \underline{Covariance:} $\sigma_{xy} = E[(X-\mu_X)(Y-\mu_Y)]=E(xy)-\mu_x\mu_y$. If $\sigma_{XY}>0$ then as $X$ increases $Y$ increases. If $\sigma_{XY}<0$ then as $X$ increases $Y$ decreases.\\
            \underline{Correlation Coefficient:} $\rho_{XY}=\frac{\sigma_{XY}}{\sigma_X\sigma_Y}$.\\
            \underline{Expected Value Sum:} $E[g(X)\pm h(Y)] = E[g(X)]\pm E[h(Y)]$. Same goes for $g(X,Y)$ and $h(X,Y)$.\\
            \underline{Independent Expected:} $E(XY) = E(X)E(Y)$ if $X$ and $Y$ are independent. That also means $\sigma_{XY}=0$.\\
            \underline{Sigma Stuff:} $\sigma_{aX+bY+c}^{2}=a^2\sigma_X^2+b^2\sigma_Y^2+2ab\sigma_{XY}$.\\
            \underline{Chebyshev's Theorem:} The probability that a random variable $X$ will be within $k$ standard deviations of the mean is $P(\mu - k\sigma<X<\mu +k\sigma)\geq 1 - \frac{1}{k^2y}$.
        \end{multicols}
    \noindent\textbf{Example Questions} 
        \begin{multicols}{2}
            \noindent 1. Answering true-false (q)uestions = $2^{q}$.\\
            2. Ordering (c)ouples who sit together = $c!\cdot 2^c$.\\
            3. Find $P(B'\cap C) = P(A\cap B'\cap C)+P(A'\cap B'\cap C) = P(A)P(B'|A)P(C|A\cap B') + P(A')P(B'|A')P(C|A'\cap B')$.\\
            4. Probability of choosing $x$ defective from 20 computers and 3 defective = $P(X = x) = \frac{\binom{3}{x}\binom{17}{2-x}}{\binom{20}{2}}$\\
            5. Discrete: $P(X>t)=1-P(X\leq t)$; $P(X\leq t)=F(t)$; $P(a<X\leq b)=P(X\leq b)-P(X\leq a)$; $P(X\leq A\mid X\geq b) = \frac{P(b\leq X\leq a)}{P(X\geq b)}$.\\
            6. There are 5 independent components, each of which possesses an operational probability of 0.92. The system does not fail if 3 out of the 5 components are operational. What is the probability that the total system is operational?
            $P(X\geq 3)=P(X=3)+P(X=4)+P(X=5)$; $P(X=3) = \binom{5}{3}(0.92)^3(1-0.92)^2 = 0.49836...$;
            $P(X=4) = \binom{5}{4}(0.92)^4(1-0.92) = 0.28656...$;
            $P(X=5) = \binom{5}{5}(0.92)^5 = 0.65908...$;
            $P(X\geq 3)=0.99547$
        \end{multicols}
\end{document}